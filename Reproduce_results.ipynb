{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "import joblib\n",
    "\n",
    "from scipy.optimize import fsolve\n",
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, ADASYN, SMOTE\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "# from ccdc.io import EntryReader # csd lisence\n",
    "# from Crystal_structure.aquire_features.feature_extraction_1 import get_pubchem_features\n",
    "# from CSD_scrapping.CSDHandler import DatasetScrapper\n",
    "# from CSD_scrapping.DatasetCleaning import DatasetEditor\n",
    "# from Crystal_structure.CSD_scrapping.DatasetVisuals import DatasetFeatureReducer\n",
    "\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# from aquire_features.feature_extraction_1 import FeatureExtractor, load_pickle_with_mol_dict\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from pulearn import ElkanotoPuClassifier # WeightedElkanotoPuClassifier,\n",
    "\n",
    "COLOR_WHEEL = ['#FFD700', '#FFB14E', '#FA8775', '#EA5F94', '#CD34B5', '#9D02D7', '#0000FF', '#7695FF'] # two last colors!!!!!!!!!!!!\n",
    "COLOR_WHEEL_2 = ['#089099', '#7CCBA2', '#FCDE9C', '#F0746E', '#7C1D6F']\n",
    "\n",
    "Y_column = ['Polymorph_flag']\n",
    "ID_columns = ['CSD_ID', 'Smiles']\n",
    "Base_columns = Y_column + ID_columns\n",
    "\n",
    "csd_feather_filename = r'C:\\Users\\itaro\\OneDrive\\Documents\\GitHub\\Crystal_structure\\database_files\\CSD_dataset_V2.ftr'\n",
    "\n",
    "\n",
    "features_1_filename=r'C:\\Users\\itaro\\OneDrive\\Documents\\GitHub\\Crystal_structure\\database_files\\features_1_df.ftr'\n",
    "features_2_filename=r'C:\\Users\\itaro\\OneDrive\\Documents\\GitHub\\Crystal_structure\\database_files\\features_2_df.ftr'\n",
    "features_3_filename=r'C:\\Users\\itaro\\OneDrive\\Documents\\GitHub\\Crystal_structure\\database_files\\features_3_df.ftr'\n",
    "features_4_filename=r'C:\\Users\\itaro\\OneDrive\\Documents\\GitHub\\Crystal_structure\\database_files\\features_4_df.ftr'\n",
    "features_5_filename=r'C:\\Users\\itaro\\OneDrive\\Documents\\GitHub\\Crystal_structure\\database_files\\features_5_df.ftr'\n",
    "\n",
    "plt.rcParams['font.family'] = ['Helvetica']\n",
    "\n",
    "estimators_dict = {'LG': LogisticRegression(random_state=42, max_iter=1000),\n",
    "                   'RF': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=100),\n",
    "                   'SVM': SVC(kernel='linear', random_state=42, probability=True), # probability=True\n",
    "                   'Nearest neighbors' : KNeighborsClassifier(n_neighbors=15, leaf_size=30),\n",
    "                   'MLP': MLPClassifier(hidden_layer_sizes=(512, 512), max_iter=500, random_state=42),\n",
    "                   }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "######################################\n",
    "########### General ##################\n",
    "######################################\n",
    "######################################\n",
    "\n",
    "def numpy_array_to_list(arr):\n",
    "    if isinstance(arr, np.ndarray):\n",
    "        return arr.tolist()\n",
    "    return arr\n",
    "\n",
    "def list_to_numpy_array(lst):\n",
    "    if isinstance(lst, list):\n",
    "        return np.array(lst, dtype=\"object\")\n",
    "    return lst\n",
    "\n",
    "def save_list_of_dicts(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f, default=numpy_array_to_list)\n",
    "\n",
    "def load_list_of_dicts(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        return [{k: list_to_numpy_array(v) for k, v in item.items()} for item in data]\n",
    "\n",
    "######################################\n",
    "######################################\n",
    "########### PRE PROCESS ##############\n",
    "######################################\n",
    "######################################\n",
    "\n",
    "def divide_df_by_polymorph_flag(df):\n",
    "    polymorph_mask = df['Polymorph_flag']==True\n",
    "    polymorph_df = df[polymorph_mask]\n",
    "    no_polymorph_df = df[~polymorph_mask]\n",
    "    return polymorph_df, no_polymorph_df\n",
    "\n",
    "def min_max_scale_dataframe(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "    df_copy = df.copy()\n",
    "    df_copy[df.columns] = scaled_data\n",
    "    return df_copy\n",
    "\n",
    "def split_dataframe(df, chunk_size):\n",
    "    num_chunks = len(df) // chunk_size + (len(df) % chunk_size > 0)\n",
    "    indices = np.random.permutation(len(df))\n",
    "    index_chunks = np.array_split(indices, num_chunks)\n",
    "    df_chunks = [df.iloc[idx] for idx in index_chunks]\n",
    "    return df_chunks\n",
    "\n",
    "def fit_vector_to_pu_learning(vector, pu_learning=False):\n",
    "    if isinstance(vector, pd.Series):\n",
    "        vector_copy = vector.copy()\n",
    "        if pu_learning:\n",
    "            vector_copy = vector_copy.replace({False: -1, True: 1})\n",
    "    elif isinstance(vector, np.ndarray):\n",
    "        vector_copy = vector.copy()\n",
    "        if pu_learning:\n",
    "            vector_copy = np.where(vector_copy == True, 1, vector_copy)\n",
    "            vector_copy = np.where(vector_copy == False, -1, vector_copy)\n",
    "    return vector_copy\n",
    "    # series_copy = series.copy()\n",
    "    # if pu_learning:\n",
    "    #     series_copy = series_copy.replace({False: -1, True: 1})\n",
    "    # return series_copy\n",
    "\n",
    "def X_y_split(df, y_column_name):\n",
    "    X = df.drop(columns=y_column_name)\n",
    "    y = df[y_column_name]\n",
    "    return X, y\n",
    "    \n",
    "def preprocess_df_for_evaluation(model_df, pu_learning=False):\n",
    "    copy_df = model_df.copy()\n",
    "    if pu_learning:\n",
    "        copy_df['Polymorph_flag'] = fit_vector_to_pu_learning(copy_df['Polymorph_flag'], pu_learning=pu_learning)\n",
    "    X_total, y_total = X_y_split(copy_df, y_column_name='Polymorph_flag')\n",
    "    polymorph_df, non_polymorph_df = divide_df_by_polymorph_flag(copy_df)\n",
    "    return X_total, y_total, polymorph_df, non_polymorph_df\n",
    "\n",
    "######################################\n",
    "######################################\n",
    "########### TRAIN## ##################\n",
    "######################################\n",
    "######################################\n",
    "\n",
    "def train_and_use_estimator(estimator, X, y, pu_learning=False, predict_method='predict', test_size=0.25, random_state=42, stratify=True):\n",
    "    if stratify:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    if pu_learning:\n",
    "        estimator = ElkanotoPuClassifier(estimator, hold_out_ratio=0.2)\n",
    "    estimator.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "    train_results = {'trained_estimator': estimator, 'y_train': y_train, 'y_train_pred': estimator.predict(X_train),\n",
    "                                                     'y_test': y_test, 'y_test_pred': estimator.predict(X_test),\n",
    "    }\n",
    "    if predict_method == 'predict_proba':\n",
    "        y_prob_pred = estimator.predict_proba(X_test.to_numpy())[:, 1]\n",
    "        train_results.update({'y_prob_pred': y_prob_pred})\n",
    "    return train_results\n",
    "\n",
    "######################################\n",
    "######################################\n",
    "########### POST ANALYSIS ############\n",
    "######################################\n",
    "######################################\n",
    "    \n",
    "def get_y_from_results(train_results, mode):\n",
    "    if mode == 'sub_train' or mode == 'os_train' or mode == 'rc_train' or mode == 'consensus_train':\n",
    "        y = train_results.get('y_train')\n",
    "        y_pred = train_results.get('y_train_pred')\n",
    "    elif mode == 'sub_test' or mode == 'os_test' or mode == 'rc_test' or mode ==  'consensus_test':\n",
    "        y = train_results.get('y_test')\n",
    "        y_pred = train_results.get('y_test_pred')\n",
    "    elif mode == 'full':\n",
    "        y = train_results.get('y_full')\n",
    "        y_pred = train_results.get('y_full_pred')\n",
    "    elif mode == 'ensamble':\n",
    "        y = train_results.get('y_ensamble')\n",
    "        y_pred = train_results.get('y_ensamble_pred')\n",
    "    elif mode == 'os_val':\n",
    "        y = train_results.get('y_val')\n",
    "        y_pred = train_results.get('y_val_pred')\n",
    "    else:\n",
    "        y, y_pred = None, None\n",
    "    return y, y_pred    \n",
    "\n",
    "def get_auc_score(y, y_pred, predict_method):\n",
    "    if predict_method == 'predict_proba':\n",
    "        auc = roc_auc_score(y, y_pred)\n",
    "        y = np.where(y > 0.5, 1, 0)\n",
    "        y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "    else:\n",
    "        auc = 0\n",
    "    return y, y_pred, auc    \n",
    "\n",
    "def get_recall_acc_sp(y, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()    \n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        recall = np.nan_to_num(tp / (tp + fn), nan=0.0)\n",
    "        accuracy = np.nan_to_num((tp + tn) / (tp + tn + fp + fn), nan=0.0)\n",
    "        specificity = np.nan_to_num(tn / (tn + fp), nan=0.0)\n",
    "    return recall, accuracy, specificity\n",
    "\n",
    "def get_all_metrics(model_results, predict_method='predict', mode='sub_train'):\n",
    "    y, y_pred = get_y_from_results(model_results, mode)\n",
    "    y, y_pred, auc = get_auc_score(y, y_pred, predict_method)\n",
    "    recall, accuracy, specificity = get_recall_acc_sp(y, y_pred)\n",
    "    pos_percent = round(sum(y_pred) / len(y_pred), 4)\n",
    "    return auc, recall, accuracy, specificity, pos_percent\n",
    "\n",
    "\n",
    "######################################\n",
    "######################################\n",
    "########### TRACKER ##################\n",
    "######################################\n",
    "######################################\n",
    "\n",
    "def eval_method_to_base_modes(eval_method):\n",
    "    base_modes = None\n",
    "    if eval_method == 'cv':\n",
    "        base_modes = ['sub_train', 'sub_test', 'full']\n",
    "    elif eval_method == 'ensamble':\n",
    "        base_modes = ['ensamble']\n",
    "    elif eval_method == 'os_SMOTE':\n",
    "        base_modes = ['os_train', 'os_test', 'os_val']\n",
    "    elif eval_method == 'random_columns':\n",
    "        base_modes = ['rc_train', 'rc_test']\n",
    "    elif eval_method == 'consensus_cv':\n",
    "        base_modes = ['consensus_train', 'consensus_test']\n",
    "    return base_modes\n",
    "\n",
    "def get_tracker_keys(mode):\n",
    "    if mode == 'sub_train':\n",
    "        tracker_keys = ['sub_train_recall', 'sub_train_accuracy', 'sub_train_specificity', 'sub_train_ROCAUC', 'sub_train_pos_percent']\n",
    "    elif mode == 'sub_test':\n",
    "        tracker_keys = ['sub_test_recall', 'sub_test_accuracy', 'sub_test_specificity', 'sub_test_ROCAUC', 'sub_test_pos_percent']\n",
    "    elif mode == 'full':\n",
    "        tracker_keys = ['full_recall', 'full_accuracy', 'full_specificity', 'full_ROCAUC', 'full_pos_percent']\n",
    "    elif mode == 'ensamble':\n",
    "        tracker_keys = ['ensamble_recall', 'ensamble_accuracy', 'ensamble_specificity', 'ensamble_ROCAUC', 'ensamble_pos_percent']\n",
    "    elif mode == 'os_train':\n",
    "        tracker_keys = ['os_train_recall', 'os_train_accuracy', 'os_train_specificity', 'os_train_ROCAUC', 'os_train_pos_percent']\n",
    "    elif mode == 'os_test':\n",
    "        tracker_keys = ['os_test_recall', 'os_test_accuracy', 'os_test_specificity', 'os_test_ROCAUC', 'os_test_pos_percent']\n",
    "    elif mode == 'os_val':\n",
    "        tracker_keys = ['os_val_recall', 'os_val_accuracy', 'os_val_specificity', 'os_val_ROCAUC', 'os_val_pos_percent']\n",
    "    elif mode == 'rc_train':\n",
    "        tracker_keys = ['rc_train_recall', 'rc_train_accuracy', 'rc_train_specificity', 'rc_train_ROCAUC', 'rc_train_pos_percent']\n",
    "    elif mode == 'rc_test':\n",
    "        tracker_keys = ['rc_test_recall', 'rc_test_accuracy', 'rc_test_specificity', 'rc_test_ROCAUC', 'rc_test_pos_percent']\n",
    "    elif mode == 'consensus_train':\n",
    "        tracker_keys = ['consensus_train_recall', 'consensus_train_accuracy', 'consensus_train_specificity', 'consensus_train_ROCAUC', 'consensus_train_pos_percent']\n",
    "    elif mode == 'consensus_test':\n",
    "        tracker_keys = ['consensus_test_recall', 'consensus_test_accuracy', 'consensus_test_specificity', 'consensus_test_ROCAUC', 'consensus_test_pos_percent']\n",
    "    else:\n",
    "        tracker_keys = None\n",
    "    return tracker_keys\n",
    "\n",
    "def setup_tracker(modes):\n",
    "    tracker = dict()\n",
    "    for mode in modes:\n",
    "        tracker_keys = get_tracker_keys(mode)\n",
    "        for key in tracker_keys:\n",
    "            tracker[key] = []\n",
    "    return tracker\n",
    "\n",
    "def update_tracker(tracker, train_results, predict_method='predict', mode='sub_train'):\n",
    "    auc, recall, accuracy, specificity, pos_percent = get_all_metrics(train_results, predict_method, mode)\n",
    "    tracker_keys = get_tracker_keys(mode)\n",
    "    for key, value in zip(tracker_keys, [recall, accuracy, specificity, auc, pos_percent]):\n",
    "        tracker[key].append(value)\n",
    "    return tracker\n",
    "\n",
    "def extract_values_from_tracker(tracker, mode):\n",
    "    keys = get_tracker_keys(mode)\n",
    "    recall = tracker.get(keys[0])\n",
    "    accuracy = tracker.get(keys[1])\n",
    "    specificty = tracker.get(keys[2])\n",
    "    auc = tracker.get(keys[3])\n",
    "    pos_percent = tracker.get(keys[4])\n",
    "    return {'Recall': recall, 'Accuracy': accuracy, 'Specificty': specificty,\n",
    "            'ROC-AUC': auc, 'Positive percentile': pos_percent}\n",
    "    # return recall, accuracy, specificty, auc, pos_percent\n",
    "\n",
    "def get_result_keys(eval_method):\n",
    "    result_keys = []\n",
    "    if eval_method == 'cv':\n",
    "        result_keys = ['train_results', 'test_results', 'full_results']\n",
    "    elif eval_method == 'ensamble':\n",
    "        result_keys = ['ensamble_results']\n",
    "    elif eval_method == 'os_SMOTE':\n",
    "        result_keys = ['train_results', 'test_results', 'validation_results']\n",
    "    elif eval_method == 'random_columns' or eval_method == 'consensus_cv':\n",
    "        result_keys = ['train_results', 'test_results']\n",
    "    return result_keys\n",
    "\n",
    "def unpack_tracker_by_eval_method(tracker, eval_method):\n",
    "    results_dict = unpack_tracker_by_eval_method(tracker, 'cv') if eval_method =='ensamble' else dict()\n",
    "    result_keys = get_result_keys(eval_method)\n",
    "    base_modes = eval_method_to_base_modes(eval_method)\n",
    "    for key, mode in zip(result_keys, base_modes):\n",
    "        results_dict[key] = extract_values_from_tracker(tracker, mode)\n",
    "    return results_dict\n",
    "\n",
    "######################################\n",
    "######################################\n",
    "########### OVER SAMPLE ##############\n",
    "######################################\n",
    "######################################\n",
    "\n",
    "def resample_cost(X, y, method='SMOTE', random_state=42):\n",
    "    if method=='SMOTE':\n",
    "        sampler = SMOTE(random_state=random_state)\n",
    "    elif method=='ADASYN':\n",
    "        sampler = ADASYN(random_state=random_state)\n",
    "    else:\n",
    "        sampler = RandomOverSampler(random_state=random_state)\n",
    "    X_res, y_res = sampler.fit_resample(X, y)\n",
    "    return X_res, y_res        \n",
    "\n",
    "def over_samp(model_df, estimator, resample_method='SMOTE', predict_method='predict', pu_learning=False):\n",
    "    base_modes = eval_method_to_base_modes('os_SMOTE')\n",
    "    X_total, y_total, *_ = preprocess_df_for_evaluation(model_df, pu_learning)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_total, y_total, test_size=0.25, random_state=42, stratify=y_total)\n",
    "    X_res, y_res = resample_cost(X_train, y_train, method=resample_method)\n",
    "    tracker = setup_tracker(modes=base_modes) # , predict_method=predict_method\n",
    "    train_results = train_and_use_estimator(estimator=estimator, X=X_res, y=y_res, pu_learning=pu_learning,\n",
    "                                            predict_method=predict_method, test_size=0.25, random_state=42, stratify=True)\n",
    "    for mode in base_modes:\n",
    "        if mode == 'os_val':\n",
    "            trn_est = train_results.get('trained_estimator')\n",
    "            val_results = {'y_val': y_val, 'y_val_pred': trn_est.predict(X_val)}\n",
    "            tracker = update_tracker(tracker=tracker, train_results=val_results, predict_method=predict_method, mode=mode)\n",
    "        else:\n",
    "            tracker = update_tracker(tracker=tracker, train_results=train_results, predict_method=predict_method, mode=mode)\n",
    "    return tracker\n",
    "\n",
    "######################################\n",
    "######################################\n",
    "########### UNDER SAMPLE #############\n",
    "######################################\n",
    "######################################\n",
    "\n",
    "def undr_samp_cv(model_df, estimator, predict_method='predict', pu_learning=False): # \n",
    "    trained_estimators = []\n",
    "    base_modes = eval_method_to_base_modes('cv')\n",
    "    X_full, y_full, polymorph_df, non_polymorph_df = preprocess_df_for_evaluation(model_df, pu_learning)\n",
    "    sampled_size = len(polymorph_df)\n",
    "    non_polymorph_chunks = split_dataframe(non_polymorph_df, sampled_size)\n",
    "    tracker = setup_tracker(modes=base_modes+['ensamble']) # , predict_method=predict_method\n",
    "    for chunk_df in non_polymorph_chunks:\n",
    "        combined_df = pd.concat([polymorph_df, chunk_df])\n",
    "        X, y = X_y_split(combined_df, y_column_name='Polymorph_flag')\n",
    "        y = fit_vector_to_pu_learning(y, pu_learning)\n",
    "        train_results = train_and_use_estimator(estimator=estimator, X=X, y=y, pu_learning=pu_learning,\n",
    "                                                predict_method=predict_method, test_size=0.25, random_state=42, stratify=True)\n",
    "        for mode in base_modes:\n",
    "            if mode == 'full':\n",
    "                trn_est = train_results.get('trained_estimator')\n",
    "                trained_estimators.append(trn_est)\n",
    "                full_results = {'y_full': y_full, 'y_full_pred': trn_est.predict(X_full)}\n",
    "                tracker = update_tracker(tracker=tracker, train_results=full_results, predict_method=predict_method, mode=mode)\n",
    "            else:\n",
    "                tracker = update_tracker(tracker=tracker, train_results=train_results, predict_method=predict_method, mode=mode)\n",
    "    return tracker, trained_estimators\n",
    "\n",
    "class CustomEnsembleClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, fitted_estimators):\n",
    "        self.fitted_estimators = fitted_estimators\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([est.predict(X) for _, est in self.fitted_estimators])\n",
    "        maj_vote = np.apply_along_axis(lambda x: Counter(x).most_common(1)[0][0], axis=0, arr=predictions)\n",
    "        return maj_vote\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        probas = [est.predict_proba(X) for _, est in self.fitted_estimators if hasattr(est, \"predict_proba\")]\n",
    "        if not probas:\n",
    "            raise AttributeError(\"None of the base estimators support predict_proba.\")\n",
    "        return np.mean(probas, axis=0)\n",
    "\n",
    "def undr_samp_ensamble(model_df, estimator, predict_method='predict', pu_learning=False):\n",
    "    base_modes = eval_method_to_base_modes('ensamble')\n",
    "    X_total, y_total, *_ = preprocess_df_for_evaluation(model_df, pu_learning)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_total, y_total, test_size=0.25, random_state=42, stratify=y_total)\n",
    "    train_df = pd.concat([X_train, y_train], axis=1)\n",
    "    tracker, trained_estimators = undr_samp_cv(train_df, estimator, predict_method, pu_learning)\n",
    "    named_estimator_list = [('estimator_{}'.format(idx), estimator) for idx, estimator in enumerate(trained_estimators)]\n",
    "    ensambler = CustomEnsembleClassifier(named_estimator_list)\n",
    "    ensamble_results = {'y_ensamble': y_test.to_numpy(), 'y_ensamble_pred': ensambler.predict(X_test)}\n",
    "    tracker = update_tracker(tracker=tracker, train_results=ensamble_results, predict_method=predict_method, mode=base_modes[0]) # \n",
    "    return tracker\n",
    "\n",
    "\n",
    "######################################\n",
    "######################################\n",
    "########### Consensus  CV  ###########\n",
    "######################################\n",
    "######################################\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.base import clone\n",
    "\n",
    "class ConsensusNestedCV:\n",
    "    def __init__(self, estimator, n_features_to_select, tracker, predict_method='predict', pu_learning=False, boot_strap=False, param_grid=None, \n",
    "                 outer_cv=5, inner_cv=5, grid_cv=3, consensus_threshold=0.6, random_state=42):\n",
    "        self.estimator = estimator\n",
    "        self.n_features_to_select = n_features_to_select\n",
    "        self.feature_selector = SelectKBest(score_func=f_classif, k=self.n_features_to_select)\n",
    "        self.tracker = tracker\n",
    "        self.predict_method = predict_method\n",
    "        self.pu_learning = pu_learning\n",
    "        self.boot_strap = boot_strap\n",
    "        self.param_grid = param_grid\n",
    "        self.outer_cv = outer_cv\n",
    "        self.inner_cv = inner_cv\n",
    "        self.grid_cv = grid_cv\n",
    "        self.consensus_threshold = consensus_threshold\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        y = fit_vector_to_pu_learning(y, self.pu_learning)\n",
    "        cv_outer = KFold(n_splits=self.outer_cv, shuffle=True, random_state=self.random_state)\n",
    "        outer_scores = []\n",
    "        consensus_features_list = []\n",
    "        for con_idx, (train_idx, test_idx) in enumerate(cv_outer.split(X)):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            cv_inner = KFold(n_splits=self.inner_cv, shuffle=True, random_state=self.random_state)\n",
    "            feature_counts = np.zeros(X_train.shape[1])\n",
    "            for inner_train_idx, inner_val_idx in cv_inner.split(X_train):\n",
    "                X_inner_train = X_train[inner_train_idx]\n",
    "                y_inner_train = y_train[inner_train_idx]\n",
    "                self.feature_selector.fit(X_inner_train, y_inner_train)\n",
    "                selected_features = self.feature_selector.get_support()\n",
    "                feature_counts += selected_features\n",
    "                # Possible to also use validation score \n",
    "\n",
    "            consensus_features = feature_counts >= (self.consensus_threshold * self.inner_cv)\n",
    "            if np.sum(consensus_features) == 0: # Fallback: select top features if no consensus\n",
    "                top_k = min(self.n_features_to_select, X_train.shape[1] // 10)\n",
    "                top_indices = np.argsort(feature_counts)[-top_k:]\n",
    "                consensus_features = np.zeros(X_train.shape[1], dtype=bool)\n",
    "                consensus_features[top_indices] = True\n",
    "            \n",
    "            # Train final model on consensus features\n",
    "            X_train_selected = X_train[:, consensus_features]\n",
    "            X_test_selected = X_test[:, consensus_features]\n",
    "            if self.param_grid:\n",
    "                grid_search = GridSearchCV(\n",
    "                    self.estimator, self.param_grid, \n",
    "                    cv=self.grid_cv, scoring='accuracy'\n",
    "                )\n",
    "                grid_search.fit(X_train_selected, y_train)\n",
    "                final_model = grid_search.best_estimator_\n",
    "            else:\n",
    "                final_model = self.estimator\n",
    "                if self.pu_learning:\n",
    "                    final_model = ElkanotoPuClassifier(final_model, hold_out_ratio=0.2)\n",
    "                final_model.fit(X_train_selected, y_train)\n",
    "            \n",
    "            base_modes = eval_method_to_base_modes('consensus_cv')\n",
    "            for mode in base_modes:\n",
    "                if 'train' in mode:\n",
    "                    outter_train_results = {'y_train': y_train, 'y_train_pred': final_model.predict(X_train_selected)}\n",
    "                    self.tracker = update_tracker(self.tracker, outter_train_results, self.predict_method, mode)\n",
    "                elif 'test' in mode:\n",
    "                    outter_test_results = {'y_test': y_test, 'y_test_pred': final_model.predict(X_test_selected)}\n",
    "                    self.tracker = update_tracker(self.tracker, outter_test_results, self.predict_method, mode)\n",
    "\n",
    "            consensus_features_list.append(consensus_features)\n",
    "        \n",
    "        self.outer_scores_ = outer_scores\n",
    "        self.consensus_features_list_ = consensus_features_list\n",
    "        self.mean_score_ = np.mean(outer_scores)\n",
    "        self.std_score_ = np.std(outer_scores)\n",
    "        return self\n",
    "\n",
    "def consensus_cv_eval(model_df, estimator, n_features_to_select, predict_method='predict', pu_learning=False):\n",
    "    base_modes = eval_method_to_base_modes('consensus_cv')\n",
    "    X_total, y_total, *_ = preprocess_df_for_evaluation(model_df, pu_learning)\n",
    "    tracker = setup_tracker(modes=base_modes)\n",
    "    consens_cv_estimator = ConsensusNestedCV(estimator=estimator, n_features_to_select=n_features_to_select, tracker=tracker, predict_method=predict_method,\n",
    "                                             pu_learning=pu_learning, outer_cv=15)\n",
    "    consens_cv_estimator.fit(X_total.to_numpy(), y_total.to_numpy())\n",
    "    updated_tracker = consens_cv_estimator.tracker\n",
    "    return updated_tracker\n",
    "\n",
    "######################################\n",
    "######################################\n",
    "########### RANDOM COLUMNS ###########\n",
    "######################################\n",
    "######################################\n",
    "\n",
    "def random_columns_eval(model_df, estimator, n_columns, n_repeats=100, predict_method='predict', pu_learning=False):\n",
    "    base_modes = eval_method_to_base_modes('random_columns')\n",
    "    X_total, y_total, *_ = preprocess_df_for_evaluation(model_df, pu_learning)\n",
    "    tracker = setup_tracker(modes=base_modes)\n",
    "    feature_columns = list(X_total.columns)\n",
    "    for _ in range(n_repeats):\n",
    "        selected_columns = random.sample(feature_columns, n_columns)\n",
    "        train_results = train_and_use_estimator(estimator=estimator, X=X_total[selected_columns], y=y_total,\n",
    "                                                pu_learning=pu_learning, predict_method=predict_method, test_size=0.25,\n",
    "                                                random_state=42, stratify=True)\n",
    "        for mode in base_modes:\n",
    "            tracker = update_tracker(tracker=tracker, train_results=train_results, predict_method=predict_method, mode=mode)\n",
    "    return tracker\n",
    "\n",
    "######################################\n",
    "######################################\n",
    "######### FULL RUN WRAPPERS ##########\n",
    "######################################\n",
    "######################################\n",
    "\n",
    "def get_import_f_name(feature_set_num):\n",
    "    if feature_set_num == 1:\n",
    "        f_name = r'C:\\Users\\itaro\\OneDrive\\Documents\\GitHub\\Crystal_structure\\database_files\\features_1_df.ftr'\n",
    "    elif feature_set_num == 2:\n",
    "        f_name = r'C:\\Users\\itaro\\OneDrive\\Documents\\GitHub\\Crystal_structure\\database_files\\features_2_df.ftr'\n",
    "    elif feature_set_num == 3:\n",
    "        f_name = r'C:\\Users\\itaro\\OneDrive\\Documents\\GitHub\\Crystal_structure\\database_files\\features_3_df.ftr'\n",
    "    elif feature_set_num == 4:\n",
    "        f_name = r'C:\\Users\\itaro\\OneDrive\\Documents\\GitHub\\Crystal_structure\\database_files\\features_4_df.ftr'\n",
    "    elif feature_set_num == 5:\n",
    "        f_name = r'C:\\Users\\itaro\\OneDrive\\Documents\\GitHub\\Crystal_structure\\database_files\\features_5_df.ftr'\n",
    "    return f_name\n",
    "\n",
    "def get_model_df(feature_set_num):\n",
    "    ID_columns = ['CSD_ID', 'Smiles']\n",
    "    f_name = get_import_f_name(feature_set_num)\n",
    "    features_df = pd.read_feather(f_name)\n",
    "    model_df = features_df.drop(ID_columns, axis=1)\n",
    "    feature_columns = list(model_df.columns[1:])\n",
    "    model_df[feature_columns] = min_max_scale_dataframe(model_df[feature_columns])\n",
    "    return model_df\n",
    "\n",
    "def save_data(data, f_name = 'try.pkl'):\n",
    "    joblib.dump(data, f_name)\n",
    "\n",
    "def load_data(f_name):\n",
    "    loaded_data = joblib.load(f_name)\n",
    "    return loaded_data\n",
    "\n",
    "def get_feature_set_results(feature_set_num, estimators_dict, eval_methods=['cv'], predict_method='predict',pu_learning=False):\n",
    "    model_df = get_model_df(feature_set_num)\n",
    "    results_dict = dict()\n",
    "    for estimator_name, estimator in estimators_dict.items():\n",
    "        full_metrics_dict = dict()\n",
    "        for eval_method in eval_methods:\n",
    "            if eval_method == 'cv':\n",
    "                metrics_dict = undr_samp_cv(model_df, estimator, predict_method, pu_learning)[0]\n",
    "            elif eval_method == 'ensamble':\n",
    "                metrics_dict = undr_samp_ensamble(model_df, estimator, predict_method, pu_learning)\n",
    "            elif eval_method == 'os_SMOTE' and estimator_name != 'SVM' and estimator_name != 'MLP':\n",
    "                metrics_dict = over_samp(model_df, estimator, 'SMOTE', predict_method, pu_learning)\n",
    "            elif eval_method == 'random_columns':\n",
    "                metrics_dict = random_columns_eval(model_df, estimator, 150, 100, predict_method, pu_learning)\n",
    "            elif eval_method == 'consensus_cv':\n",
    "                metrics_dict = consensus_cv_eval(model_df, estimator, 150, predict_method, pu_learning)\n",
    "            full_metrics_dict = {**full_metrics_dict, **metrics_dict} \n",
    "        results_dict[estimator_name] = full_metrics_dict\n",
    "    return results_dict\n",
    "\n",
    "def get_eval_methods(result_keys):\n",
    "    eval_methods = set()\n",
    "    for key in result_keys:\n",
    "        if key.startswith('sub'):\n",
    "            eval_methods.add('cv')\n",
    "        elif key.startswith('ensamble'):\n",
    "            eval_methods.add('ensamble')\n",
    "        elif key.startswith('os'):\n",
    "            eval_methods.add('os_SMOTE')\n",
    "        elif key.startswith('rc'):\n",
    "            eval_methods.add('random_columns')\n",
    "        elif key.startswith('consensus'):\n",
    "            eval_methods.add('consensus_cv')\n",
    "    if 'cv' in eval_methods and 'ensamble' in eval_methods:\n",
    "        eval_methods.remove('cv')\n",
    "    return list(eval_methods)\n",
    "\n",
    "def unpack_saved_results(f_name):\n",
    "    final_dict = dict()\n",
    "    feature_set_results_dict = load_data(f_name)\n",
    "    for feature_set_num, result_by_algo in feature_set_results_dict.items():\n",
    "        for algo_name, results_dict in result_by_algo.items():\n",
    "            eval_methods = get_eval_methods(results_dict.keys())\n",
    "            for eval_method in eval_methods:\n",
    "                inner_results = unpack_tracker_by_eval_method(results_dict, eval_method)\n",
    "                if eval_method not in final_dict:\n",
    "                    final_dict[eval_method] = dict()\n",
    "                if feature_set_num not in final_dict[eval_method]:\n",
    "                    final_dict[eval_method][feature_set_num] = dict()\n",
    "                final_dict[eval_method][feature_set_num][algo_name] = inner_results\n",
    "    return final_dict\n",
    "\n",
    "def unpack_saved_results_full_by_fset(f_name):\n",
    "    final_dict = dict()  # eval_method → feature_set_num → metric_key → inner_metric_key → algo_name\n",
    "    feature_set_results_dict = load_data(f_name)\n",
    "    for feature_set_num, result_by_algo in feature_set_results_dict.items():\n",
    "        for algo_name, results_dict in result_by_algo.items():\n",
    "            eval_methods = get_eval_methods(results_dict.keys())\n",
    "            for eval_method in eval_methods:\n",
    "                inner_results = unpack_tracker_by_eval_method(results_dict, eval_method)\n",
    "                for metric_key, metric_dict in inner_results.items():\n",
    "                    for inner_metric_key, inner_metric_value in metric_dict.items():\n",
    "                        final_dict.setdefault(eval_method, {}) \\\n",
    "                                  .setdefault(feature_set_num, {}) \\\n",
    "                                  .setdefault(metric_key, {}) \\\n",
    "                                  .setdefault(inner_metric_key, {})[algo_name] = inner_metric_value\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "def unpack_saved_results_full_by_algo(f_name):\n",
    "    final_dict = dict()  # eval_method → algo_name → metric_key → feature_set_num → inner_metric_key\n",
    "    feature_set_results_dict = load_data(f_name)\n",
    "    for feature_set_num, result_by_algo in feature_set_results_dict.items():\n",
    "        for algo_name, results_dict in result_by_algo.items():\n",
    "            eval_methods = get_eval_methods(results_dict.keys())\n",
    "            for eval_method in eval_methods:\n",
    "                inner_results = unpack_tracker_by_eval_method(results_dict, eval_method)\n",
    "                for metric_key, metric_dict in inner_results.items():\n",
    "                    for inner_metric_key, inner_metric_value in metric_dict.items():\n",
    "                        final_dict.setdefault(eval_method, {}) \\\n",
    "                                  .setdefault(algo_name, {}) \\\n",
    "                                  .setdefault(metric_key, {}) \\\n",
    "                                  .setdefault(feature_set_num, {})[inner_metric_key] = inner_metric_value\n",
    "                        \n",
    "    return final_dict\n",
    "\n",
    "def merge_eval_methods_at_feature_set_level(final_dict, eval_method_1, eval_method_2, merged_eval_method_name):\n",
    "    merged_dict = {}\n",
    "    dict1 = final_dict.get(eval_method_1, {})\n",
    "    dict2 = final_dict.get(eval_method_2, {})\n",
    "    all_algos = set(dict1.keys()) | set(dict2.keys())\n",
    "    for algo_name in all_algos:\n",
    "        metrics1 = dict1.get(algo_name, {})\n",
    "        metrics2 = dict2.get(algo_name, {})\n",
    "        all_metrics = set(metrics1.keys()) | set(metrics2.keys())\n",
    "        for metric_key in all_metrics:\n",
    "            fs_dict1 = metrics1.get(metric_key, {})\n",
    "            fs_dict2 = metrics2.get(metric_key, {})\n",
    "            combined_fs_dict = {}\n",
    "            # Add all feature_set_num entries from both methods\n",
    "            for fs_num, inner_dict in fs_dict1.items():\n",
    "                combined_fs_dict[fs_num] = inner_dict.copy()\n",
    "            for fs_num, inner_dict in fs_dict2.items():\n",
    "                if fs_num in combined_fs_dict:\n",
    "                    # Optional: handle collision case\n",
    "                    raise ValueError(\n",
    "                        f\"Feature set '{fs_num}' appears in both eval methods under \"\n",
    "                        f\"{algo_name} → {metric_key}\"\n",
    "                    )\n",
    "                combined_fs_dict[fs_num] = inner_dict.copy()\n",
    "            # Store into merged_dict\n",
    "            merged_dict \\\n",
    "                .setdefault(merged_eval_method_name, {}) \\\n",
    "                .setdefault(algo_name, {}) \\\n",
    "                .setdefault(metric_key, {}) \\\n",
    "                .update(combined_fs_dict)\n",
    "    return merged_dict\n",
    "\n",
    "def swap_train_test(method_dict):\n",
    "    new_method_dict = method_dict\n",
    "    for k, v_dict in method_dict.items():\n",
    "        temp_holder = v_dict.get('train_results')\n",
    "        new_method_dict[k]['train_results'] = v_dict.get('test_results')\n",
    "        new_method_dict[k]['test_results'] = temp_holder\n",
    "    return new_method_dict\n",
    "\n",
    "def update_ensamble_smote(summary_dict):\n",
    "    summary_dict['ensamble'] = swap_train_test(summary_dict.get('ensamble'))\n",
    "    summary_dict['os_SMOTE'] = swap_train_test(summary_dict.get('os_SMOTE'))\n",
    "    return summary_dict\n",
    "\n",
    "\n",
    "######################################\n",
    "######################################\n",
    "############# PLOTTING ###############\n",
    "######################################\n",
    "######################################\n",
    "\n",
    "def get_method_dict(fset_num, full_summary_dict, method='oversampling'):\n",
    "    if fset_num<=3:\n",
    "        if method == 'oversampling':\n",
    "            method_dict = full_summary_dict.get('os_SMOTE')\n",
    "        elif method == 'undersampling':\n",
    "            method_dict = full_summary_dict.get('ensamble')\n",
    "    else:\n",
    "        method = 'consensus_cv'\n",
    "        method_dict = full_summary_dict.get('consensus_cv')\n",
    "    return method_dict\n",
    "\n",
    "def get_fset_table(fset_num, full_summary_dict, method='oversampling'):\n",
    "    method_dict = get_method_dict(fset_num, full_summary_dict, method)\n",
    "    fset_dict = method_dict.get(fset_num)\n",
    "    overall_df = pd.DataFrame()\n",
    "    for result_type, result_dict in fset_dict.items():\n",
    "        type_prefix = result_type.split('_')[0]+'_'\n",
    "        # if result_type == 'train_results':\n",
    "        mean_results = []\n",
    "        for metric_name, metric_dict in result_dict.items():\n",
    "            mean_metric_dict = dict()\n",
    "            for algo_name, algo_values in metric_dict.items():\n",
    "                mean_metric_dict[algo_name] = np.mean(algo_values)\n",
    "            mean_results.append(mean_metric_dict)\n",
    "        df = pd.DataFrame(mean_results, index=result_dict.keys()).T\n",
    "        col_order = ['Accuracy', 'ROC-AUC', 'Specificty', 'Recall']\n",
    "        reduced_df = df[col_order]\n",
    "        reduced_df.columns = [type_prefix+col_name for col_name in reduced_df.columns]\n",
    "        overall_df = pd.concat([overall_df, reduced_df], axis=1)\n",
    "    return overall_df\n",
    "\n",
    "def get_algo_table(algo_name, full_summary_dict, method_name = 'combined'):\n",
    "    pre_dict = full_summary_dict.get(method_name)\n",
    "    algo_dict = pre_dict.get(algo_name)\n",
    "    overall_df = pd.DataFrame()\n",
    "    result_types = ['train_results', 'test_results', 'validation_results', 'ensamble_results'] # 'full_results'\n",
    "    for result_type in result_types:\n",
    "        result_dict = algo_dict.get(result_type)\n",
    "        if result_dict:\n",
    "            type_prefix = result_type.split('_')[0] + '_'\n",
    "            mean_results = []\n",
    "            for fset_dict in result_dict.values():\n",
    "                mean_metric_dict = dict()\n",
    "                for metric_name, metric_values in fset_dict.items():\n",
    "                    mean_metric_dict[metric_name] = np.mean(metric_values)    \n",
    "                mean_results.append(mean_metric_dict)\n",
    "            df = pd.DataFrame(mean_results, index=result_dict.keys())\n",
    "            col_order = ['Accuracy', 'ROC-AUC', 'Specificty', 'Recall']\n",
    "            reduced_df = df[col_order]\n",
    "            reduced_df.columns = [type_prefix+col_name for col_name in reduced_df.columns]\n",
    "            overall_df = pd.concat([overall_df, reduced_df], axis=1)\n",
    "    return overall_df\n",
    "\n",
    "from ptitprince import half_violinplot, stripplot\n",
    "\n",
    "def create_axes(ax_title_list):\n",
    "    final_axes = {}\n",
    "    for title in ax_title_list:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(14.5, 13.5))\n",
    "        for direction in ['right', 'left', 'top', 'bottom']:\n",
    "            ax.spines[direction].set_visible(False)\n",
    "        ax.set_title(title, fontdict={'fontsize': 25})\n",
    "        final_axes[title] = (fig, ax)\n",
    "    return final_axes\n",
    "\n",
    "def split_plot_data(plot_data, mode = 'ensamble'):\n",
    "    train_results_dict = plot_data.get('train_results')\n",
    "    test_results_dict = plot_data.get('test_results')\n",
    "    if mode == 'ensamble':\n",
    "        ensamble_results_dict = plot_data.get('ensamble_results')\n",
    "        full_results_dict = plot_data.get('full_results')\n",
    "        return train_results_dict, test_results_dict, full_results_dict, ensamble_results_dict\n",
    "    elif mode == 'os':\n",
    "        validation_results_dict = plot_data.get('validation_results')\n",
    "        return train_results_dict, test_results_dict, validation_results_dict\n",
    "    return train_results_dict, test_results_dict\n",
    "\n",
    "\n",
    "def transform_dict_to_cat_df(results_dict):\n",
    "    init_df = pd.DataFrame(results_dict)\n",
    "    df_long = init_df.melt(var_name='Algorithm', value_name='metric_value')\n",
    "    df_long['Algorithm'] = pd.Categorical(df_long['Algorithm'], \n",
    "                                     categories=results_dict.keys(), \n",
    "                                     ordered=True)\n",
    "    return df_long\n",
    "\n",
    "def plot_raincloud(results_dict, ax=None, color='black', alpha=1):\n",
    "    df_form = transform_dict_to_cat_df(results_dict)\n",
    "    half_violinplot(data=df_form, x='metric_value', y='Algorithm', ax=ax, bw=0.2, inner=None, cut=0., orient='h', linewidth=1.5, color=color, width = .7) # scale='area', , alpha=alpha\n",
    "    stripplot(data=df_form, x='metric_value', y='Algorithm', ax=ax, jitter=0.1, orient='h', size=5, color=color)\n",
    "\n",
    "def plot_raincloud_strip(ensamble_dict, ax=None, color='black', alpha=1):\n",
    "    df_form = transform_dict_to_cat_df(ensamble_dict)\n",
    "    stripplot(data=df_form, x='metric_value', y='Algorithm', ax=ax, jitter=0.1, orient='h', size=100, move=0, color=color, marker='$|$', alpha=alpha)\n",
    "\n",
    "def plot_ensamble_results_raincloud(plot_data, axes = None, colors = ['orange', 'blue']):\n",
    "    train_results_dict, test_results_dict, full_results_dict, ensamble_results_dict = split_plot_data(plot_data, mode = 'ensamble')\n",
    "    metric_list = list(train_results_dict.keys())\n",
    "    if axes is None:\n",
    "        axes = create_axes(metric_list)\n",
    "    for metric_name, (fig, ax) in axes.items():\n",
    "        plot_raincloud(train_results_dict.get(metric_name), ax=ax, color=colors[0]) # 'cyan' , alpha=1\n",
    "        plot_raincloud(test_results_dict.get(metric_name), ax=ax, color=colors[1]) # 'gold' , alpha=0.7\n",
    "    return axes\n",
    "\n",
    "def plot_os_results_raincloud(plot_data, axes = None):\n",
    "    train_results_dict, test_results_dict, validation_results_dict = split_plot_data(plot_data, mode = 'os')\n",
    "    metric_list = list(train_results_dict.keys())\n",
    "    if axes is None:\n",
    "        axes = create_axes(metric_list) \n",
    "    for metric_name, (fig, ax) in axes.items():\n",
    "        plot_raincloud_strip(train_results_dict.get(metric_name), ax=ax, color='orange')\n",
    "        plot_raincloud_strip(test_results_dict.get(metric_name), ax=ax, color='blue')\n",
    "    return axes\n",
    "\n",
    "def plot_random_columns_results_raincloud(plot_data, axes = None, colors = ['orange', 'blue']):\n",
    "    train_results_dict, test_results_dict = split_plot_data(plot_data, mode = 'random_columns')\n",
    "    metric_list = list(train_results_dict.keys())\n",
    "    if axes is None:\n",
    "        axes = create_axes(metric_list)\n",
    "    for metric_name, (fig, ax) in axes.items():\n",
    "        plot_raincloud(train_results_dict.get(metric_name), ax=ax, color=colors[0])\n",
    "        plot_raincloud(test_results_dict.get(metric_name), ax=ax, color=colors[1])\n",
    "    return axes\n",
    "\n",
    "def scale_plots(axes, feature_set_num):\n",
    "    category_list = ['LG', 'RF', 'Nearest neighbors', 'SVM', 'MLP']\n",
    "    for metric, (fig, ax) in axes.items(): \n",
    "        ax.set_yticks(range(len(category_list)))\n",
    "        ax.set_yticklabels(category_list)\n",
    "        ax.set_ylim([5,-1])\n",
    "        ax.set_xlim([0,1])\n",
    "        ax.set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1], [0, 0.2, 0.4, 0.6, 0.8, 1], fontsize=25)\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(f'new_summary_US_raincloud_feature_set_{feature_set_num}_{metric}.png', dpi=1000, bbox_inches='tight')\n",
    "\n",
    "class RainDropPlotter:\n",
    "    def __init__(self, line_colors=None):\n",
    "        self.line_colors = line_colors if line_colors else ['#72EFDD', '#64DFDF', '#56CFE1', '#4EA8DE', '#5390D9']\n",
    "        self.axes = None\n",
    "\n",
    "    def gen_plot_by_fset(self, summary_dict, fset_num, pu_summary_dict=None):\n",
    "        if fset_num<=3:\n",
    "            ensamble_data = summary_dict.get('ensamble')\n",
    "            os_smote_data = summary_dict.get('os_SMOTE')\n",
    "            if ensamble_data:\n",
    "                by_metric_dict = ensamble_data.get(fset_num)\n",
    "                self.axes = plot_ensamble_results_raincloud(by_metric_dict, self.axes)\n",
    "                if pu_summary_dict:\n",
    "                    pu_ensamble_data = pu_summary_dict.get('ensamble')\n",
    "                    pu_by_metric_dict = pu_ensamble_data.get(fset_num)\n",
    "                    self.axes = plot_ensamble_results_raincloud(pu_by_metric_dict, self.axes, colors = ['#5390D9', '#AE2012'])\n",
    "            else:\n",
    "                return None\n",
    "            if os_smote_data:\n",
    "                by_metric_dict = os_smote_data.get(fset_num)\n",
    "                self.axes = plot_os_results_raincloud(by_metric_dict, self.axes)\n",
    "            scale_plots(self.axes, fset_num)\n",
    "        else:\n",
    "            consensus_cv_data = summary_dict.get('consensus_cv')\n",
    "            by_metric_dict = consensus_cv_data.get(fset_num)\n",
    "            if consensus_cv_data:\n",
    "                self.axes = plot_random_columns_results_raincloud(by_metric_dict, self.axes)\n",
    "                if pu_summary_dict:\n",
    "                    pu_consensus_cv_data = pu_summary_dict.get('consensus_cv')\n",
    "                    pu_by_metric_dict = pu_consensus_cv_data.get(fset_num)\n",
    "                    self.axes = plot_random_columns_results_raincloud(pu_by_metric_dict, self.axes, colors = ['#5390D9', '#AE2012'])\n",
    "            scale_plots(self.axes, fset_num)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.patches import Circle, RegularPolygon\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.projections import register_projection\n",
    "from matplotlib.projections.polar import PolarAxes\n",
    "from matplotlib.spines import Spine\n",
    "from matplotlib.transforms import Affine2D\n",
    "\n",
    "\n",
    "def radar_factory(num_vars, frame='circle'):\n",
    "    \"\"\"\n",
    "    Create a radar chart with `num_vars` Axes.\n",
    "\n",
    "    This function creates a RadarAxes projection and registers it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_vars : int\n",
    "        Number of variables for radar chart.\n",
    "    frame : {'circle', 'polygon'}\n",
    "        Shape of frame surrounding Axes.\n",
    "\n",
    "    \"\"\"\n",
    "    # calculate evenly-spaced axis angles\n",
    "    theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n",
    "\n",
    "    class RadarTransform(PolarAxes.PolarTransform):\n",
    "\n",
    "        def transform_path_non_affine(self, path):\n",
    "            # Paths with non-unit interpolation steps correspond to gridlines,\n",
    "            # in which case we force interpolation (to defeat PolarTransform's\n",
    "            # autoconversion to circular arcs).\n",
    "            if path._interpolation_steps > 1:\n",
    "                path = path.interpolated(num_vars)\n",
    "            return Path(self.transform(path.vertices), path.codes)\n",
    "\n",
    "    class RadarAxes(PolarAxes):\n",
    "\n",
    "        name = 'radar'\n",
    "        PolarTransform = RadarTransform\n",
    "\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            # rotate plot such that the first axis is at the top\n",
    "            self.set_theta_zero_location('N')\n",
    "\n",
    "        def fill(self, *args, closed=True, **kwargs):\n",
    "            \"\"\"Override fill so that line is closed by default\"\"\"\n",
    "            return super().fill(closed=closed, *args, **kwargs)\n",
    "\n",
    "        def plot(self, *args, **kwargs):\n",
    "            \"\"\"Override plot so that line is closed by default\"\"\"\n",
    "            lines = super().plot(*args, **kwargs)\n",
    "            for line in lines:\n",
    "                self._close_line(line)\n",
    "\n",
    "        def _close_line(self, line):\n",
    "            x, y = line.get_data()\n",
    "            # FIXME: markers at x[0], y[0] get doubled-up\n",
    "            if x[0] != x[-1]:\n",
    "                x = np.append(x, x[0])\n",
    "                y = np.append(y, y[0])\n",
    "                line.set_data(x, y)\n",
    "\n",
    "        def set_varlabels(self, labels):\n",
    "            self.set_thetagrids(np.degrees(theta), labels)\n",
    "\n",
    "        def _gen_axes_patch(self):\n",
    "            # The Axes patch must be centered at (0.5, 0.5) and of radius 0.5\n",
    "            # in axes coordinates.\n",
    "            if frame == 'circle':\n",
    "                return Circle((0.5, 0.5), 0.5)\n",
    "            elif frame == 'polygon':\n",
    "                return RegularPolygon((0.5, 0.5), num_vars,\n",
    "                                      radius=.5, edgecolor=\"k\")\n",
    "            else:\n",
    "                raise ValueError(\"Unknown value for 'frame': %s\" % frame)\n",
    "\n",
    "        def _gen_axes_spines(self):\n",
    "            if frame == 'circle':\n",
    "                return super()._gen_axes_spines()\n",
    "            elif frame == 'polygon':\n",
    "                # spine_type must be 'left'/'right'/'top'/'bottom'/'circle'.\n",
    "                spine = Spine(axes=self,\n",
    "                              spine_type='circle',\n",
    "                              path=Path.unit_regular_polygon(num_vars))\n",
    "                # unit_regular_polygon gives a polygon of radius 1 centered at\n",
    "                # (0, 0) but we want a polygon of radius 0.5 centered at (0.5,\n",
    "                # 0.5) in axes coordinates.\n",
    "                spine.set_transform(Affine2D().scale(.5).translate(.5, .5)\n",
    "                                    + self.transAxes)\n",
    "                return {'polar': spine}\n",
    "            else:\n",
    "                raise ValueError(\"Unknown value for 'frame': %s\" % frame)\n",
    "\n",
    "    register_projection(RadarAxes)\n",
    "    return theta\n",
    "\n",
    "class RadarPlotter:\n",
    "    def __init__(self, axs_names, line_colors=None, scale_colors=None):\n",
    "        self.axs_names = axs_names # '#874CCC' '#C65BCF' \n",
    "        self.line_colors = line_colors if line_colors else [\"#B5D5EE\", '#56CFE1', '#5390D9', '#C65BCF', '#874CCC'] # ['#72EFDD', '#64DFDF', '#56CFE1', '#4EA8DE', '#5390D9']\n",
    "        self.scale_colors = scale_colors if scale_colors else ['#E9D8A6', '#EE9B00', '#CA6702', '#BB3E03', '#AE2012']\n",
    "\n",
    "    def init_axs(self, nrows=1, ncols=1): # \n",
    "        self.N = len(self.axs_names) \n",
    "        self.theta = radar_factory(self.N, frame='polygon')\n",
    "        self.fig, self.axs = plt.subplots(figsize=(5*ncols, 24*nrows), nrows=nrows, ncols=ncols, #\n",
    "                            subplot_kw=dict(projection='radar'))\n",
    "        self.fig.subplots_adjust(wspace=0.5, hspace=0.25, top=0.85, bottom=0.05)\n",
    "\n",
    "    def init_ax(self, ax, ax_title=None):\n",
    "        ax.set_ylim([0,1])\n",
    "        ax.set_rgrids([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "        ax.set_rlabel_position(90)\n",
    "        # ax.set_title(ax_title) \n",
    "        scale_positions = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "        for color, scale_point in zip(self.scale_colors, scale_positions):\n",
    "            ax.plot(self.theta, [scale_point]*self.N, color=color, linewidth=2, alpha=0.4)\n",
    "\n",
    "    def finish_design(self, ax):\n",
    "        ax.set_varlabels([None]*4)\n",
    "        for label in ax.get_yticklabels():\n",
    "            label.set_visible(False)\n",
    "        # ax.set_varlabels(self.axs_names)\n",
    "        # ax.set_facecolor(\"lightpink\")\n",
    "\n",
    "    def gen_plot_by_eval_method(self, summary_dict, eval_method):\n",
    "        estimator_names = ['LG', 'RF', 'SVM', 'Nearest neighbors', 'MLP']\n",
    "        method_dict = summary_dict.get(eval_method)\n",
    "        for estimator_name in estimator_names:\n",
    "            if estimator_name in method_dict.keys():\n",
    "                s_estimator_dict = method_dict.get(estimator_name)\n",
    "                base_types = ['train_results', 'test_results']\n",
    "                ordered_keys = base_types + [k for k in s_estimator_dict if k not in base_types]\n",
    "                if 'full_results' in ordered_keys:\n",
    "                    ordered_keys.remove('full_results')\n",
    "                self.init_axs(ncols=len(ordered_keys))\n",
    "                ordered_items = [(k, s_estimator_dict[k]) for k in ordered_keys]\n",
    "                for ax, (metric_type, metric_dict) in zip(self.axs.flat, ordered_items):\n",
    "                    self.init_ax(ax, metric_type)\n",
    "                    for color, (fset_num, fset_dict) in zip(self.line_colors, metric_dict.items()):\n",
    "                        mean_label_result = [np.mean(fset_dict.get(label)) for label in self.axs_names]\n",
    "                        if fset_num<=3: # or fset_num==5: # fset_num<=3\n",
    "                            ax.plot(self.theta, mean_label_result, color=color, linewidth=5, alpha=1-0.03*fset_num) # , alpha=1-0.07*plot_idx\n",
    "                        # else:\n",
    "                        #     ax.plot(self.theta, mean_label_result, color=color, linewidth=5, alpha=1-0.03*fset_num, linestyle='--')\n",
    "                    self.finish_design(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators_dict = {'LG': LogisticRegression(random_state=42, max_iter=1000),\n",
    "                   'RF': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=100),\n",
    "                   'SVM': SVC(kernel='linear', random_state=42, probability=True),\n",
    "                   'Nearest neighbors' : KNeighborsClassifier(n_neighbors=15, leaf_size=30),\n",
    "                   'MLP': MLPClassifier(hidden_layer_sizes=(512, 512), max_iter=500, random_state=42),\n",
    "                   }\n",
    "\n",
    "feature_set_dict = {\n",
    "                    1: ['ensamble', 'os_SMOTE'],\n",
    "                    2: ['ensamble', 'os_SMOTE'],\n",
    "                    3: ['ensamble', 'os_SMOTE'],\n",
    "                    4: ['consensus_cv'],\n",
    "                    5: ['consensus_cv'],\n",
    "                    }\n",
    "\n",
    "######################################\n",
    "######################################\n",
    "######### Normal evaluation ##########\n",
    "######################################\n",
    "######################################\n",
    "\n",
    "feature_set_results = dict()\n",
    "for feature_set_num, eval_modes in feature_set_dict.items():\n",
    "    results = get_feature_set_results(feature_set_num, estimators_dict, pu_learning=False, eval_methods=eval_modes, predict_method='predict_proba')\n",
    "    feature_set_results[feature_set_num] = results\n",
    "\n",
    "save_data(feature_set_results, 'full_normal_eval.pkl')\n",
    "\n",
    "######################################\n",
    "######################################\n",
    "########### PU evaluation ############\n",
    "######################################\n",
    "######################################\n",
    "\n",
    "feature_set_results = dict()\n",
    "for feature_set_num, eval_modes in feature_set_dict.items():\n",
    "    results = get_feature_set_results(feature_set_num, estimators_dict, pu_learning=True, eval_methods=eval_modes, predict_method='predict_proba')\n",
    "    feature_set_results[feature_set_num] = results\n",
    "\n",
    "save_data(feature_set_results, 'full_pu_eval.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and init process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_ensamble_summary_dict = unpack_saved_results_full_by_algo(r'C:\\Users\\itaro\\OneDrive\\Documents\\GitHub\\Crystal_structure\\full_normal_eval.pkl')\n",
    "algo_ensamble_summary_dict = update_ensamble_smote(algo_ensamble_summary_dict)\n",
    "algo_concv_summary_dict = unpack_saved_results_full_by_algo(r'C:\\Users\\itaro\\OneDrive\\Documents\\GitHub\\Crystal_structure\\fset_45_normal_eval.pkl')\n",
    "algo_full_summary_dict = {**algo_ensamble_summary_dict, **algo_concv_summary_dict}\n",
    "us_algo_full_summary_dict = merge_eval_methods_at_feature_set_level(algo_full_summary_dict, 'ensamble', 'consensus_cv', 'combined')\n",
    "os_algo_full_summary_dict = merge_eval_methods_at_feature_set_level(algo_full_summary_dict, 'os_SMOTE', 'consensus_cv', 'combined')\n",
    "\n",
    "pu_algo_ensamble_summary_dict = unpack_saved_results_full_by_algo(r'C:\\Users\\itaro\\OneDrive\\Documents\\GitHub\\Crystal_structure\\full_pu_eval.pkl')\n",
    "pu_algo_ensamble_summary_dict = update_ensamble_smote(pu_algo_ensamble_summary_dict)\n",
    "pu_algo_concv_summary_dict = unpack_saved_results_full_by_algo(r'C:\\Users\\itaro\\OneDrive\\Documents\\GitHub\\Crystal_structure\\fset_45_pu_eval.pkl')\n",
    "pu_algo_full_summary_dict = {**pu_algo_ensamble_summary_dict, **pu_algo_concv_summary_dict}\n",
    "pu_us_algo_full_summary_dict = merge_eval_methods_at_feature_set_level(pu_algo_full_summary_dict, 'ensamble', 'consensus_cv', 'combined')\n",
    "pu_os_algo_full_summary_dict = merge_eval_methods_at_feature_set_level(pu_algo_full_summary_dict, 'os_SMOTE', 'consensus_cv', 'combined')\n",
    "\n",
    "fset_ensamble_summary_dict = unpack_saved_results_full_by_fset(r'C:\\Users\\itaro\\OneDrive\\Documents\\GitHub\\Crystal_structure\\full_normal_eval.pkl')\n",
    "fset_ensamble_summary_dict = update_ensamble_smote(fset_ensamble_summary_dict)\n",
    "fset_concv_summary_dict = unpack_saved_results_full_by_fset(r'C:\\Users\\itaro\\OneDrive\\Documents\\GitHub\\Crystal_structure\\fset_45_normal_eval.pkl')\n",
    "fset_full_summary_dict = {**fset_ensamble_summary_dict, **fset_concv_summary_dict}\n",
    "\n",
    "pu_fset_ensamble_summary_dict = unpack_saved_results_full_by_fset(r'C:\\Users\\itaro\\OneDrive\\Documents\\GitHub\\Crystal_structure\\full_pu_eval.pkl')\n",
    "pu_fset_ensamble_summary_dict = update_ensamble_smote(pu_fset_ensamble_summary_dict)\n",
    "pu_fset_concv_summary_dict = unpack_saved_results_full_by_fset(r'C:\\Users\\itaro\\OneDrive\\Documents\\GitHub\\Crystal_structure\\fset_45_pu_eval.pkl')\n",
    "pu_fset_full_summary_dict = {**pu_fset_ensamble_summary_dict, **pu_fset_concv_summary_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per fset tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fset_method_combi = [(1, 'oversampling'), (1, 'undersampling'),\n",
    "                     (2, 'oversampling'), (2, 'undersampling'),\n",
    "                     (3, 'oversampling'), (3, 'undersampling'),\n",
    "                     (4, 'consensus_cv'), (5, 'consensus_cv'),]\n",
    "for fset_num, method in fset_method_combi:\n",
    "    fset_df = get_fset_table(fset_num, fset_full_summary_dict, method)\n",
    "    table_name = f'fset_{fset_num}_{method}.csv'\n",
    "    fset_df.to_csv(table_name)\n",
    "    pu_fset_df = get_fset_table(fset_num, pu_fset_full_summary_dict, method)\n",
    "    pu_table_name = 'pu_'+table_name\n",
    "    pu_fset_df.to_csv(pu_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per algo tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_algo_list = ['LG', 'RF', 'Nearest neighbors'] \n",
    "for algo_name in os_algo_list:\n",
    "    us_df = get_algo_table(algo_name, us_algo_full_summary_dict)\n",
    "    os_df = get_algo_table(algo_name, os_algo_full_summary_dict)\n",
    "    us_pu_df = get_algo_table(algo_name, pu_us_algo_full_summary_dict)\n",
    "    os_pu_df = get_algo_table(algo_name, pu_os_algo_full_summary_dict)\n",
    "    df_list = [os_df.loc[1:3, :], os_pu_df.loc[1:3, :], us_df.loc[1:3, :], us_pu_df.loc[1:3, :]]\n",
    "    united_df = pd.concat(df_list, axis=0)\n",
    "    df_list2 = [os_df.loc[4, :], os_pu_df.loc[4, :]]\n",
    "    mini_df = pd.concat(df_list2, axis=1).T\n",
    "    united_df = pd.concat([united_df, mini_df])\n",
    "    united_df.to_csv(f'{algo_name}_algo_table.csv')\n",
    "\n",
    "us_algo_list = ['LG', 'RF', 'Nearest neighbors', 'SVM', 'MLP'] # \n",
    "for algo_name in us_algo_list:\n",
    "    us_df = get_algo_table(algo_name, us_algo_full_summary_dict)\n",
    "    us_pu_df = get_algo_table(algo_name, pu_us_algo_full_summary_dict)\n",
    "    df_list = [us_df.loc[1:3, :], us_pu_df.loc[1:3, :]]\n",
    "    united_df = pd.concat(df_list, axis=0)\n",
    "    df_list2 = [us_df.loc[4, :], us_pu_df.loc[4, :]]\n",
    "    mini_df = pd.concat(df_list2, axis=1).T\n",
    "    united_df = pd.concat([united_df, mini_df])\n",
    "    united_df.to_csv(f'{algo_name}_algo_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raindrop plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_drop_plotter = RainDropPlotter()\n",
    "rain_drop_plotter.gen_plot_by_fset(fset_full_summary_dict, fset_num=4, pu_summary_dict=pu_fset_full_summary_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_method = 'combined'\n",
    "metrics = ['Accuracy', 'Specificty','ROC-AUC', 'Recall']\n",
    "radar_plotter = RadarPlotter(metrics)\n",
    "radar_plotter.gen_plot_by_eval_method(pu_os_algo_full_summary_dict, eval_method) \n",
    "# ['LG', 'RF', 'SVM', 'Nearest neighbors', 'MLP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Water filled plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View CSD data and duplicate reduction process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csd_df = pd.read_feather(csd_feather_filename)\n",
    "\n",
    "# Aggregate the data by bins of 5 years\n",
    "bins = range(csd_df['Publication_year'].min(), csd_df['Publication_year'].max()+6, 5)\n",
    "counts, _ = np.histogram(csd_df['Publication_year'], bins=bins)\n",
    "\n",
    "# Create the bar chart\n",
    "# plt.bar(bins[:-2], counts[:-1], width=4)\n",
    "# plt.bar(bins[:-2], counts[:-1].sum(), width=4)\n",
    "# plt.xlabel('Publication Year')\n",
    "# plt.ylabel('Number of Entries')\n",
    "# plt.title('Number of Entries Added Every 5 Years')\n",
    "plt.bar(bins[:-2], counts[:-1], width=4)\n",
    "plt.bar(bins[:-2], np.cumsum(counts[:-1]), width=4, alpha=0.5)\n",
    "plt.xlabel('Publication Year')\n",
    "plt.ylabel('Number of Entries')\n",
    "plt.title('Number of Entries Added Every 5 Years')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csd_df = pd.read_feather(csd_feather_filename)\n",
    "dataset_editor=DatasetEditor(csd_df)\n",
    "rmae_size_values=[]\n",
    "# possible_cufoff_values = [10**(idx) for idx in range(-8, 6)]\n",
    "possible_cufoff_values = [10**(-5), 10**(-4), 10**(-3), 10**(-2), 0.02, 0.04, 0.06, 0.08, 10**(-1), 0.2, 0.4, 0.6, 0.8, 10**(0), 10**(1), 10**(2)]\n",
    "for relative_mae_cutoff in possible_cufoff_values:\n",
    "    dataset_editor.reduce_relative_MAE_entries(relative_mae_cutoff=relative_mae_cutoff, verbose=False)\n",
    "    original_dataset = dataset_editor.dataset\n",
    "    dataset_editor.dataset = dataset_editor.kept_dataset\n",
    "    dataset_editor.calculate_polymorph_properties(polymorph_method=POLYMORPH_METHOD)\n",
    "    dataset_editor.dataset = original_dataset\n",
    "    rmae_size_values.append((relative_mae_cutoff, dataset_editor.removed_size, dataset_editor.has_polymorph_percent))\n",
    "\n",
    "rMAE_PLOT_SIZE = (6.4, 4.8)\n",
    "rMAE_AX_POSITION = (0.1,0.1,0.5,0.8)\n",
    "\n",
    "relative_mae_values, removed_size_values, polymorph_percent=zip(*rmae_size_values)\n",
    "fig = plt.figure(figsize=rMAE_PLOT_SIZE)\n",
    "ax = plt.axes(rMAE_AX_POSITION)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.scatter(relative_mae_values, removed_size_values, marker='o', c='#7C1D6F')\n",
    "plt.tick_params(left = False, bottom = False) \n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Relative MAE cutoff')\n",
    "ax.set_ylabel('Number of removed crystals')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
